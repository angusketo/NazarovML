{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b28b07a",
   "metadata": {},
   "source": [
    "Conda environment: pygeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07935789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random,math,os,copy\n",
    "random.seed(0)\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.rdDistGeom import GetMoleculeBoundsMatrix\n",
    "\n",
    "# Pytorch and Pytorch Geometric\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as Fun\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch.nn import Linear\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f55c484",
   "metadata": {},
   "source": [
    "# 1) Functions\n",
    "## 1.1) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d413a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(x,ls:list)->list:\n",
    "    \"\"\"Take an input x and list and return equally sized list but with 1 and 0 depending \n",
    "    on if the input x matches the elements in the list\"\"\"\n",
    "    return [int(x==e) for e in ls]\n",
    "\n",
    "def bondfeatures(bond:rdkit.Chem.rdchem.Bond,dmatrix:np.ndarray,featuredic:dict={'bondtype':True,\n",
    "                'conjugated':True,\n",
    "                'inring':True,\n",
    "                'stereochemistry':True,\n",
    "                'bondlength':True}\n",
    "               )->np.ndarray:\n",
    "    \"\"\"Given a bond, distance matrix, and dictionary of desired features, return a numpy array of bond features\"\"\"\n",
    "    bond_features = []\n",
    "    if featuredic['bondtype']: ### onehot for bond order\n",
    "        bond_features += onehot(bond.GetBondType(),[Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE, Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC])\n",
    "    if featuredic['conjugated']: ### 1/0 if bond is conjugated\n",
    "        bond_features += [int(bond.GetIsConjugated())]\n",
    "    if featuredic['inring']: ### 1/0 if bond is in a ring\n",
    "        bond_features += [int(bond.IsInRing())]\n",
    "    if featuredic['stereochemistry']: ### onehot for bond stereochemistry\n",
    "        bond_features += onehot(bond.GetStereo(),[\"STEREOZ\", \"STEREOE\", \"STEREOANY\", \"STEREONONE\"])\n",
    "    if featuredic['bondlength']: ### Using a distance matrix containing bond lengths, get the mean bond length for bond A-B and B-A\n",
    "        avgbond = np.mean([dmatrix[bond.GetBeginAtom().GetIdx()-1][bond.GetEndAtom().GetIdx()-1],dmatrix[bond.GetEndAtom().GetIdx()-1][bond.GetBeginAtom().GetIdx()-1]])\n",
    "        bond_features += [(avgbond-bmin)/(bmax-bmin)]\n",
    "\n",
    "    return np.array(bond_features) \n",
    "\n",
    "def atomfeatures2(atom:rdkit.Chem.rdchem.Atom, \n",
    "                 featuredic:dict = {'atomtypeonehot':True,\n",
    "                 'useHs':False,\n",
    "                 'heavyatomValence':True,\n",
    "                 'formalcharge':True,\n",
    "                 'hybridization':True,\n",
    "                 'ringmembership':True,\n",
    "                 'aromatic':True,\n",
    "                 'electronegativity':True,\n",
    "                 'atompolarizability':True,\n",
    "                 'gasteiger':True,\n",
    "                 'mass':True,\n",
    "                 'vdw':True,\n",
    "                 'covalent':True,\n",
    "                 'chirality':True, \n",
    "                 'hydrogencount':True},\n",
    "                 atomls:list = ['C','N','O','S','F','Si','P','Cl','Br','Unknown']\n",
    "                      )->np.ndarray: \n",
    "    \"\"\"Given an atom, dictionary of desired features, and atom types, produce a feature vector.\"\"\"\n",
    "    atom_feature_vector = []\n",
    "    if featuredic['atomtypeonehot']: ### Atom type. ex: H:0, C:1, F:0, etc.\n",
    "        if featuredic['useHs']:\n",
    "            atomls = ['H'] + atomls\n",
    "        atom_feature_vector += onehot(str(atom.GetSymbol()), atomls)\n",
    "    \n",
    "    if featuredic['heavyatomValence']: ### Atom valency\n",
    "        atom_feature_vector += [int(atom.GetDegree())]\n",
    "    \n",
    "    if featuredic['formalcharge']: ### Formal charge\n",
    "        atom_feature_vector += [atom.GetFormalCharge()]\n",
    "\n",
    "    if featuredic['hybridization']: ### SPx hybridization\n",
    "        atom_feature_vector += onehot(int(atom.GetHybridization()), [\"SP\", \"SP2\", \"SP3\", \"OTHER\"])\n",
    "\n",
    "    if featuredic['ringmembership']: ### Whether atom is part of a ring\n",
    "        atom_feature_vector += [int(atom.IsInRing())]\n",
    "    \n",
    "    if featuredic['aromatic']: ### Whether atom is aromatic\n",
    "        atom_feature_vector += [int(atom.GetIsAromatic())]\n",
    "    \n",
    "    if featuredic['electronegativity']: ### Electronegativity (comes from PubChem data)\n",
    "        atom_feature_vector += [enchart[str(atom.GetSymbol())]]\n",
    "    \n",
    "    if featuredic['atompolarizability']: ### Atom polarizability (comes from PubChem data)\n",
    "        atom_feature_vector += [polchart[str(atom.GetSymbol())]]\n",
    "    \n",
    "    if featuredic['gasteiger']: ### Gasteiger partial charges\n",
    "        atom_feature_vector += [(atom.GetDoubleProp('_GasteigerCharge')-cmin)/(cmax-cmin)]\n",
    "    \n",
    "    if featuredic['mass']: ### Atomic Mass\n",
    "        atom_feature_vector += [masschart[str(atom.GetSymbol())]]\n",
    "    \n",
    "    if featuredic['vdw']: ### Van der Waals radius\n",
    "        atom_feature_vector += [vdwchart[str(atom.GetSymbol())]]\n",
    "        \n",
    "    if featuredic['covalent']: ### Covalent radius \n",
    "        atom_feature_vector += [covchart[str(atom.GetSymbol())]]\n",
    "                                        \n",
    "    if featuredic['chirality'] == True: ### Chirality\n",
    "        atom_feature_vector += onehot(str(atom.GetSymbol()), [\"CHI_UNSPECIFIED\", \"CHI_TETRAHEDRAL_CW\", \"CHI_TETRAHEDRAL_CCW\", \"CHI_OTHER\"])\n",
    "    \n",
    "    if featuredic['hydrogencount'] == True: ### How many hydrogens are bonded\n",
    "        atom_feature_vector += onehot(str(atom.GetSymbol()), [0, 1, 2, 3, \"3+\"])\n",
    "    return np.array(atom_feature_vector)\n",
    "\n",
    "def create_pytorchgeometric_dataset2(x_smiles:list, y:list, pattern_smiles:list,afdic:dict,bfdic:dict,AddHs:bool=False,\n",
    "                                     neighbours:int=2)->list:\n",
    "    \"\"\"Create a list of pytorch geometric data objects.\n",
    "    x_smiles = list of molecule SMILES\n",
    "    y = calculated barrier height\n",
    "    pattern_smiles = divinyl ketone bonding patterns (multiple are needed to account for aromaticity), optional method of\n",
    "        reducing the graph size\n",
    "    afdic = atom feature dictionary\n",
    "    bfdic = bond feature dictionary\n",
    "    AddHs = whether hydrogen atoms should be considered when generating atom features\n",
    "    neighbours = When using pattern_smiles to reduce the graph size, how many neighbouring atoms to the divinyl ketone\n",
    "        reaction centre (pattern_smiles) should be included in the graph.\n",
    "    \"\"\"\n",
    "    ### Generate RDKit mol objects for the patterns if neccesary\n",
    "    if pattern_smiles != None:\n",
    "        pattern_mols = [Chem.MolFromSmiles(x) for x in pattern_smiles]\n",
    "    data_list = []\n",
    "    \n",
    "    \n",
    "    for (smiles, y_val) in zip(x_smiles, y):\n",
    "        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if pattern_smiles:\n",
    "            # Find matching pattern in the molecule. Several matches are needed to account for aromaticity\n",
    "            matches = None\n",
    "            matchfound = False\n",
    "            for p in pattern_mols:\n",
    "                matches = mol.GetSubstructMatches(p)\n",
    "                if matches:\n",
    "                    matchfound=True\n",
    "                    break \n",
    "            if not matchfound:\n",
    "                raise Exception()\n",
    "            \n",
    "            # Get atoms in the subgraph: pattern atoms + their neighbors\n",
    "            atom_indices = set(matches[0]) ### reaction centre atoms\n",
    "            for n in range(0,neighbours): ## neighbours\n",
    "                tempatoms = set()\n",
    "                ### For each atom in the reaction centre, add the indexes of all of its neighbours to the temporary set\n",
    "                [tempatoms.update([nbr.GetIdx() for nbr in mol.GetAtomWithIdx(atom).GetNeighbors()]) for atom in atom_indices]\n",
    "                atom_indices.update(tempatoms)\n",
    "        else:\n",
    "            atom_indices = list(range(0,len(mol.GetAtoms())))\n",
    "        \n",
    "        ### Prep charges and bond distances\n",
    "        if AddHs:\n",
    "            mol = Chem.AddHs(mol)\n",
    "        AllChem.ComputeGasteigerCharges(mol)\n",
    "        dmatrix = GetMoleculeBoundsMatrix(mol)\n",
    "        \n",
    "        ### Automatic feature dimensions\n",
    "        n_nodes = len(atom_indices)\n",
    "        n_node_features = len(atomfeatures2(mol.GetAtomWithIdx(0),afdic))\n",
    "        n_edge_features = len(bondfeatures(mol.GetBonds()[0],dmatrix,bfdic))\n",
    "        \n",
    "        ### Atom index dictionary for correctly mapping atom and bond features\n",
    "        atom_map = {old_idx: new_idx for new_idx, old_idx in enumerate(atom_indices)}\n",
    "\n",
    "        # Node feature matrix\n",
    "        X = np.zeros((n_nodes, n_node_features))\n",
    "        for old_idx in atom_indices:\n",
    "            atom = mol.GetAtomWithIdx(old_idx)\n",
    "            X[atom_map[old_idx], :] = atomfeatures2(atom,afdic)\n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        \n",
    "        # Edge index and features\n",
    "        edges = []\n",
    "        edge_features = []\n",
    "        for bond in mol.GetBonds():\n",
    "            start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            if start in atom_indices and end in atom_indices:\n",
    "                edges.append((atom_map[start], atom_map[end]))\n",
    "                edges.append((atom_map[end], atom_map[start]))\n",
    "                edge_features.append(bondfeatures(bond,dmatrix,bfdic))\n",
    "                edge_features.append(bondfeatures(bond,dmatrix,bfdic))\n",
    "        \n",
    "        # Edge index tensor\n",
    "        E = torch.tensor(np.array(edges).T, dtype=torch.long)\n",
    "        \n",
    "        # Edge feature tensor\n",
    "        EF = torch.tensor(np.array(edge_features), dtype=torch.float)\n",
    "        \n",
    "        # Label tensor\n",
    "        y_tensor = torch.tensor([y_val], dtype=torch.float)\n",
    "        \n",
    "        # Construct PyTorch Geometric data object\n",
    "        data = Data(x=X, edge_index=E, edge_attr=EF, y=y_tensor)\n",
    "        data_list.append(data)\n",
    "    \n",
    "    return data_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6717e1",
   "metadata": {},
   "source": [
    "## 1.2) Training, Validation, Testing, and K-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8868cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(loader, model, loss, optimizer,device:str='cuda'):\n",
    "    \"\"\"Train for one epoch\n",
    "    loader = dataloader\n",
    "    model = GNN model \n",
    "    loss = loss function\n",
    "    optimizer = optimizer function\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    current_loss = 0\n",
    "    ### To calculate MAE (for ease of analysis), keep track of ground truth and predictions\n",
    "    groundtruth = []\n",
    "    predictions = []\n",
    "    for d in loader: \n",
    "        device = torch.device(device)\n",
    "        d = d.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        d.x = d.x.float() ### convert all x features to float\n",
    "\n",
    "        out = model(d) ### make prediction\n",
    "\n",
    "        l = loss(out, torch.reshape(d.y, (len(d.y), 1))) ### calculate loss using model output and ground truth\n",
    "        \n",
    "        ### Add the data out and ground truth to cpu rather than keep on GPU\n",
    "        predictions += list(out.cpu().detach().numpy()) ### add to list of predictions\n",
    "        groundtruth += list(torch.reshape(d.y, (len(d.y), 1)).cpu().numpy()) ### add to list of ground truths\n",
    "\n",
    "        current_loss += l / len(loader) ### In case of multiple batches, the loss must be added together\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    currentMAE = mean_absolute_error(predictions,groundtruth) ### calculate MAE for ease of analysis\n",
    "    return current_loss, currentMAE, model\n",
    "\n",
    "def validation(loader, model, loss):\n",
    "    \"\"\"Validation\n",
    "    loader = dataloader\n",
    "    model = GNN model \n",
    "    loss = loss function\"\"\"\n",
    "    \n",
    "    model.eval() ### evaluation mode\n",
    "    \n",
    "    val_loss = 0\n",
    "    ### To calculate MAE (for ease of analysis), keep track of ground truth and predictions\n",
    "    groundtruth = []\n",
    "    predictions = []\n",
    "    for d in loader:\n",
    "        device = torch.device('cuda')\n",
    "        d = d.to(device)\n",
    "        out = model(d)\n",
    "        l = loss(out, torch.reshape(d.y, (len(d.y), 1)))\n",
    "        val_loss += l / len(loader)\n",
    "        \n",
    "        predictions += list(out.cpu().detach().numpy()) ### add to list of predictions\n",
    "        groundtruth += list(torch.reshape(d.y, (len(d.y), 1)).cpu().numpy()) ### add to list of ground truths\n",
    "        \n",
    "    valMAE = mean_absolute_error(predictions,groundtruth) ### calculate MAE\n",
    "    return val_loss,valMAE\n",
    "\n",
    "@torch.no_grad()\n",
    "def testing(loader, model,loss):\n",
    "    \"\"\"Testing\n",
    "    loader = dataloader\n",
    "    model = GNN model \n",
    "    loss = loss function\"\"\"\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_target = np.empty((0))\n",
    "    test_y_target = np.empty((0))\n",
    "    for d in loader:\n",
    "        device = torch.device('cuda')\n",
    "        d = d.to(device)\n",
    "        out = model(d)\n",
    "        l = loss(out, torch.reshape(d.y, (len(d.y), 1)))\n",
    "        test_loss += l / len(loader)\n",
    "\n",
    "        # save prediction vs ground truth values for later use\n",
    "        test_target = np.concatenate((test_target, out.cpu().detach().numpy()[:, 0]))\n",
    "        test_y_target = np.concatenate((test_y_target, d.y.detach().cpu().numpy()))\n",
    "\n",
    "    return test_loss, test_target, test_y_target\n",
    "\n",
    "def train_epochs(epochs, model, train_loader, val_loader, test_loader, path,printresults=None,lr=0.001, weight_decay=5e-4,loss = torch.nn.MSELoss()):\n",
    "    \"\"\"Training over all epochs\n",
    "    epochs = number of epochs\n",
    "    model = GNN model\n",
    "    train_loader = data loader for training set\n",
    "    val_loader = data loader for validation set\n",
    "    test_loader = data loader for testing set\n",
    "    path = path to save best model\n",
    "    printresults = print results every x epochs\n",
    "    lr = learning rate\n",
    "    weight_decay = decay rate in optimizer\"\"\"\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    train_loss = np.empty(epochs)\n",
    "    val_loss = np.empty(epochs)\n",
    "    test_loss = np.empty(epochs)\n",
    "    train_MAE = np.empty(epochs)\n",
    "    val_MAE = np.empty(epochs)\n",
    "    test_MAE = np.empty(epochs)\n",
    "    best_loss = math.inf\n",
    "    best_mae = math.inf\n",
    "    \n",
    "    ### These are temporary filenames that are immediately overwritten  \n",
    "    newpath='nonexistantfile'\n",
    "    newpathMAE = 'nonexistantfile'\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss, epochMAE, model = training(train_loader, model, loss, optimizer)\n",
    "        test_loss, test_target, test_y_target = testing(train_loader,model,loss)\n",
    "        v_loss,valMAE = validation(val_loader, model, loss)\n",
    "        ### Calculate testing loss\n",
    "        testMAE = mean_absolute_error(test_target, test_y_target) \n",
    "        \n",
    "        ### If model has a lower loss, save it\n",
    "        if v_loss < best_loss:\n",
    "            if os.path.exists(newpath):\n",
    "                os.remove(newpath)\n",
    "            newpath = f\"\"\"{path.split('.pt')[0]}_{epoch}_{round(float(v_loss.detach().cpu().numpy()),5)}_{round(float(valMAE),5)}.pt\"\"\"\n",
    "            torch.save(model.state_dict(), newpath)\n",
    "            best_loss = v_loss\n",
    "        ### If model has a lower MAE, save it\n",
    "        if valMAE < best_mae:\n",
    "            if os.path.exists(newpathMAE):\n",
    "                os.remove(newpathMAE)\n",
    "            newpathMAE = f\"\"\"{path.split('.pt')[0]}_{epoch}_{round(float(v_loss.detach().cpu().numpy()),5)}_{round(float(valMAE),5)}.pt\"\"\"\n",
    "            torch.save(model.state_dict(), newpathMAE)\n",
    "            best_mae = valMAE\n",
    "\n",
    "        ### Record loss\n",
    "        train_loss[epoch] = epoch_loss.cpu().detach().numpy()\n",
    "        val_loss[epoch] = v_loss.cpu().detach().numpy()\n",
    "        ### Record MAE\n",
    "        train_MAE[epoch] = epochMAE\n",
    "        val_MAE[epoch] = valMAE\n",
    "        test_MAE[epoch] = testMAE\n",
    "        \n",
    "        ### Print current train and val loss as well as train, val, testing MAE\n",
    "        if printresults !=None:\n",
    "            if epoch % printresults == 0 or epoch== epochs-1:\n",
    "                print(f\"Epoch: {str(epoch)}, Train loss: {round(epoch_loss.item(),2)}, Val loss: {round(v_loss.item(),2)}, Train MAE: {epochMAE:.2f}, Val MAE: {valMAE:.2f}, Test MAE: {testMAE:.2f}\")\n",
    "\n",
    "    ### In case the newpath and newpathMAE variables were not overwritten (can occur depending on model performance, epochs, and randomness)\n",
    "    if not os.path.exists(newpath):\n",
    "        newpath = newpathMAE\n",
    "    if not os.path.exists(newpathMAE):\n",
    "        newpathMAE = newpath\n",
    "    ### Return losses, MAEs, and paths to best models\n",
    "    return train_loss, val_loss, test_loss, train_MAE, val_MAE, test_MAE,newpath,newpathMAE\n",
    "\n",
    "def kfoldsGNN(datavalues,model,afdic,bfdic,path=None,pretraining=None,k_folds = 5,epochs = 2000,\n",
    "              loss = torch.nn.MSELoss(),manseed=0,pattern = 'C=CC(C=C)=O',batch_size=128,dim_h=128,\n",
    "              printresults=None,AddHs=False,neighbours=2,lr=0.001, weight_decay=5e-4):\n",
    "    \"\"\"\n",
    "    datavalues = list of lists/tuples containing [SMILES, barrier height]\n",
    "    model = GNN model\n",
    "    afdic = Dictionary detailing which atom/node features to use\n",
    "    bfdic = Dictionary detailing which bond/edge feature sto use\n",
    "    path = List of model paths for each fold\n",
    "    pretraining = Load a pretrained model\n",
    "    k_folds = Number of k_folds to use\n",
    "    epochs = Number of epochs to use\n",
    "    loss = Loss function to use\n",
    "    manseed = Random seed for PyTorch\n",
    "    pattern = SMILES pattern for reaction centre (used in reaction centre based GNNs)\n",
    "    batch_size = size of each batch in dataloaders\n",
    "    dim_h = number of dimensions for each hidden layer\n",
    "    printresults = print results every x epochs\n",
    "    AddHs = Whether to add hydrogens to the structures when creating graphs\n",
    "    neighbours = Number of neighbours to reaction centre to consider (used in reaction centre based GNNs)\n",
    "    lr = Learning rate\n",
    "    weight_decay = Weight decay to use in optimizer\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    torch.manual_seed(manseed)\n",
    "\n",
    "    ### K-folds split dataset\n",
    "    kfold = KFold(n_splits=k_folds,shuffle=True,random_state=0)\n",
    "\n",
    "    ### Generate graph data\n",
    "    data_list = create_pytorchgeometric_dataset2([x[0] for x in datavalues], [x[1] for x in datavalues], pattern,afdic,bfdic,AddHs=AddHs,neighbours=2)\n",
    "    \n",
    "    ### Provide some default pathnames in current directory\n",
    "    if path == None:\n",
    "        path = [\"GNN_model.pt\"]*k_folds\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(data_list)):\n",
    "        if printresults:\n",
    "            print(f'FOLD {fold}')\n",
    "            print('--------------------------------')\n",
    "        ### Split testing set to form validation set\n",
    "        valid_ids = test_ids[:int(len(test_ids)/2)]\n",
    "        test_ids = [x for x in test_ids if x not in valid_ids]\n",
    "        \n",
    "        # Sample elements randomly from a given list of ids, no replacement.\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "        ### Create data loaders for the training, validation, and testing sets\n",
    "        train_loader = DataLoader(dataset = data_list, batch_size = batch_size, sampler=train_subsampler)\n",
    "        valid_loader = DataLoader(dataset = data_list, batch_size = batch_size, sampler=valid_subsampler)    \n",
    "        test_loader = DataLoader(dataset = data_list, batch_size = batch_size, sampler=test_subsampler)    \n",
    "        \n",
    "        device = torch.device('cuda')        \n",
    "        modeltouse = model(data_list[0]['x'].shape[1],dim_h=dim_h).to(device)\n",
    "\n",
    "        if pretraining != None:\n",
    "            modeltouse.load_state_dict(torch.load(pretraining[fold],weights_only=True)).to(device)\n",
    "            print(f'Loaded model: {path[fold]}')\n",
    "        gcn_train_loss, gcn_val_loss,gcn_test_loss, gcn_train_MAE, gcn_val_MAE,gcn_test_MAE,newpath,newpathMAE = train_epochs(epochs, modeltouse, train_loader, valid_loader, test_loader, path[fold], printresults=printresults, lr=lr, weight_decay=weight_decay,loss=loss)\n",
    "        \n",
    "        ### Get Best Model\n",
    "        modeltouse.load_state_dict(torch.load(newpathMAE,weights_only=True))\n",
    "        ### Get loss using best model\n",
    "        train_loss, train_target, train_y_target = testing(train_loader,modeltouse,loss)\n",
    "        valid_loss, valid_target, valid_y_target = testing(valid_loader,modeltouse,loss)\n",
    "        test_loss, test_target, test_y_target = testing(test_loader,modeltouse,loss)\n",
    "        ### Get MAE using best model\n",
    "        trainmae = mean_absolute_error(train_target, train_y_target)\n",
    "        validmae = mean_absolute_error(valid_target, valid_y_target)\n",
    "        testmae = mean_absolute_error(test_target, test_y_target)\n",
    "        ### Print results for best model according to validation set\n",
    "        print(f'Fold {fold}, Best Validation epoch: Train MAE = {round(trainmae,2)}, Valid MAE = {round(validmae,2)}, Test MAE = {round(testmae,2)}') \n",
    "        \n",
    "        ### Record results for each set. _y_ indicates predictions\n",
    "        results[fold] = [gcn_train_loss, gcn_val_loss, gcn_train_MAE, gcn_val_MAE,trainmae,validmae,testmae,gcn_test_loss,gcn_test_MAE, valid_target, valid_y_target, test_target, test_y_target]\n",
    "\n",
    "    print('==')\n",
    "    print(f'Best Validation Model Train MAE: {np.mean([x[4] for x in results.values()]):.2f} ± {np.std([x[4] for x in results.values()]):.2f}')\n",
    "    print(f'Best Validation Model Valid MAE: {np.mean([x[5] for x in results.values()]):.2f} ± {np.std([x[5] for x in results.values()]):.2f}')\n",
    "    print(f'Best Validation Model Test MAE: {np.mean([x[6] for x in results.values()]):.2f} ± {np.std([x[6] for x in results.values()]):.2f}')\n",
    "    print('==\\n\\n')\n",
    "    return results\n",
    "\n",
    "\n",
    "def test_new_molecule(trainvalid_data,test_data,model,afdic,bfdic,path=None,pretraining=None,k_folds = 5,epochs = 2000,\n",
    "              loss = torch.nn.MSELoss(),manseed=0,pattern = 'C=CC(C=C)=O',batch_size=128,dim_h=128,\n",
    "              printresults=None,AddHs=False,neighbours=2,lr=0.001, weight_decay=5e-4):\n",
    "    \"\"\"\n",
    "    trainvalid_data = list of lists/tuples containing [SMILES, barrier height] used for trainign and validating model\n",
    "                        Use the entire dataset from the manuscript for this\n",
    "    test_data = list of lists/tuples containing [SMILES, barrier height]. This is the new unknown data\n",
    "    model = GNN model\n",
    "    afdic = Dictionary detailing which atom/node features to use\n",
    "    bfdic = Dictionary detailing which bond/edge feature sto use\n",
    "    path = List of model paths for each fold\n",
    "    pretraining = Load a pretrained model\n",
    "    k_folds = Number of k_folds to use\n",
    "    epochs = Number of epochs to use\n",
    "    loss = Loss function to use\n",
    "    manseed = Random seed for PyTorch\n",
    "    pattern = SMILES pattern for reaction centre (used to restrict graph to reaction centre)\n",
    "    batch_size = size of each batch in dataloaders\n",
    "    dim_h = number of dimensions for each hidden layer\n",
    "    printresults = print results every x epochs\n",
    "    AddHs = Whether to add hydrogens to the structures when creating graphs\n",
    "    neighbours = Number of neighbours to reaction centre to consider (used in reaction centre based GNNs)\n",
    "    lr = Learning rate\n",
    "    weight_decay = Weight decay to use in optimizer\n",
    "    \"\"\"\n",
    "    results = {x[0]:[] for x in test_data}\n",
    "    torch.manual_seed(manseed)\n",
    "\n",
    "    ### K-folds split dataset\n",
    "    kfold = KFold(n_splits=k_folds,shuffle=True,random_state=0)\n",
    "\n",
    "    ### Generate graph data\n",
    "    data_list = create_pytorchgeometric_dataset2([x[0] for x in trainvalid_data], [x[1] for x in trainvalid_data], pattern,afdic,bfdic,AddHs=AddHs,neighbours=2)\n",
    "    test_data_list = create_pytorchgeometric_dataset2([x[0] for x in test_data], [x[1] for x in test_data], pattern,afdic,bfdic,AddHs=AddHs,neighbours=2)\n",
    "    \n",
    "    ### Provide some default pathnames in current directory\n",
    "    if path == None:\n",
    "        path = [\"GNN_model.pt\"]*k_folds\n",
    "    for fold, (train_ids, valid_ids) in enumerate(kfold.split(data_list)):\n",
    "        # Sample elements randomly from a given list of ids, no replacement.\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
    "        \n",
    "        ### Create data loaders for the training, validation, and testing sets\n",
    "        train_loader = DataLoader(dataset = data_list, batch_size = batch_size, sampler=train_subsampler)\n",
    "        valid_loader = DataLoader(dataset = data_list, batch_size = batch_size, sampler=valid_subsampler)    \n",
    "        test_loader = DataLoader(dataset = test_data_list, batch_size = batch_size)    \n",
    "        \n",
    "        device = torch.device('cuda')        \n",
    "        modeltouse = model(data_list[0]['x'].shape[1],dim_h=dim_h).to(device)\n",
    "\n",
    "        if pretraining == None: ### Train models\n",
    "            gcn_train_loss, gcn_val_loss,gcn_test_loss, gcn_train_MAE, gcn_val_MAE,gcn_test_MAE,newpath,newpathMAE = train_epochs(epochs, modeltouse, train_loader, valid_loader, test_loader, path[fold], printresults=printresults, lr=lr, weight_decay=weight_decay)\n",
    "            print('Best model:',newpathMAE)\n",
    "            ### Get Best Model\n",
    "            modeltouse.load_state_dict(torch.load(newpathMAE,weights_only=True))\n",
    "        else: ### Use pretrained models\n",
    "            modeltouse.load_state_dict(torch.load(pretraining[fold],weights_only=True))\n",
    "            print(f'Loaded model: {pretraining[fold]}')\n",
    "        \n",
    "        test_loss, test_target, test_y_target = testing(test_loader,modeltouse,loss)\n",
    "        for n,x in enumerate(test_target):\n",
    "            results[test_data[n][0]] += [x]\n",
    "    \n",
    "    print('Average after K-Folds')\n",
    "    for k,v in results.items():\n",
    "        print(k,round(np.mean(v),1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486fb994",
   "metadata": {},
   "source": [
    "# 2) Training and Testing\n",
    "## 2.1) Data and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df425eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ReactionID_SMILES : [lowest_BH,U_to_TS_BH]\n",
    "## U_to_TS_BH is not neccesary but is provided for data purposes\n",
    "\n",
    "dataset = {'1_C=CC(C=C)=O':[15.8,14.8],\n",
    "           '2_C=CC(C(C)=C)=O':[13.9,12.8],\n",
    "           '3_C=C(C)C(C(C)=C)=O':[9.7,9.7],\n",
    "           '4_C=C(C)C(/C(C)=C/C)=O':[10.6,10.6],\n",
    "           '5_O=C(/C(C)=C/C)/C(C)=C/C':[11.9,11.9],\n",
    "           '6_O=C(/C(C)=C/C)/C(C)=C(C)/C':[11.1,11.1],\n",
    "           '7_O=C(/C(C)=C(C)/C)/C(C)=C(C)/C':[13.4,11.9],\n",
    "           '8_CC(C(C1=CC=CC=C1)=O)=C':[21,21],\n",
    "           '9_C/C(C)=C(C(C2=CC=CC=C2)=O)/C':[20.5,20.5],\n",
    "           '10_O=C(C3=CC=CC=C3)C4=CC=CC=C4':[30.5,30.5],\n",
    "           '11_C=C(C)C(C(OC)=C)=O':[11.4,11.4],\n",
    "           '12_C=C(C)C(C(C(OC)=O)=C)=O':[13.6,13.6],\n",
    "           '13_C=C(C)C(/C(OC)=C/C(OC)=O)=O':[9.3,9.3],\n",
    "           '14_C=C(C(OC)=O)C(C(OC)=C)=O':[11.9,11.9],\n",
    "           '15_O=C(/C(C)=C(C)/C)/C(C)=C(C)/C1=CC=CC=C1':[12.7,12.7],\n",
    "           '16_O=C(/C(C)=C(C)/C)/C(C)=C(C2=CC=CC=C2)/C':[15.2,15.1],\n",
    "           '17_C=CC(/C=C/C3=CC=CC=C3)=O':[21.3,19.4],\n",
    "           '18_C=CC(/C=C\\C4=CC=CC=C4)=O':[22.2,20.0],\n",
    "           '19_C=CC(/C(C(OC)=O)=C/OC)=O':[17.3,17.3],\n",
    "           '20_O=C(/C(C)=C(C)/OC)/C(C)=C(C)\\C':[16.3,15.5],\n",
    "           '21_O=C(/C(OC)=C(C)/C(OC)=O)/C(C)=C(C)\\C':[10.7,10.4],\n",
    "           '22_O=C(/C(OC)=C(C)/C)/C(C)=C(C)\\C':[12.0,9.8],\n",
    "           '23_O=C(/C(C(OC)=O)=C(C)/C)/C(C)=C(C)\\C':[15.6,15.1],\n",
    "           '24_O=C(/C(OC)=C(C(OC)=O)/C)/C(C)=C(C)\\C':[9.2,8.8],\n",
    "           '25_O=C(/C(C)=C\\C)/C(C)=C\\C':[15.9,15.9],\n",
    "           '26_O=C(/C=C(C)\\C)/C=C(C)\\C':[30.2,27.2],\n",
    "           '27_O=C(/C=C\\C)/C=C\\C':[27.1,24.8],\n",
    "           '28_O=C(/C=C/C)/C=C/C':[21.9,20.1],\n",
    "           '29_C=CC(C(OC)=C)=O':[15.2,12.2],\n",
    "           '30_C=CC(C(C(OC)=O)=C)=O':[18.9,17.3],\n",
    "           '31_C=CC(/C(C)=C(C)/C)=O':[16.1,14.3],\n",
    "           '32_C=CC(/C=C(C)/C)=O':[24.4,21.1],\n",
    "           '33_O=C(/C(C)=C(C)/C)/C(C)=C(CC)/C':[12.5,11.5],\n",
    "           '34_O=C(/C(C)=C(C)/C)/C(C)=C(C(C)C)/C':[14.3,13.2],\n",
    "           '35_O=C(/C(C)=C(C)/C)C1=C(C)CCCC1':[14.8,14.4],\n",
    "           '36_O=C(/C(C)=C(C)/C)C1=C(C(C)C)CCCC1':[15.2,14.3],\n",
    "           '37_C=C(C)C(C(SC)=C)=O':[7.5,7.5],\n",
    "           '38_C=C(C)C(C(C#N)=C)=O':[11.3,11.2],\n",
    "           '39_O=C(/C(C)=C(C#N)/C)/C(C)=C(C)/O':[18.6,18.2],\n",
    "           '40_O=C(/C(C)=C(C#N)/C)/C(C)=C(C)/C':[15.8,15.8],\n",
    "           '41_O=C(/C(OC)=C(C)/C)/C(C(OC)=O)=C(C)\\C':[12.0,10.5],\n",
    "           '42_O=C(/C(C)=C(C)/C#N)/C(C)=C(C)/C':[16.2,15.7],\n",
    "           '43_O=C(/C(SC)=C(C)/C)/C(C)=C(C)\\C':[11.2,9.5],\n",
    "           '44_O=C(/C(SC)=C(C(OC)=O)/C)/C(C)=C(C)\\C':[10.2,8.5],\n",
    "           '45_C/C(C)=C(C(C1=CC=CC=C1)=O)/SC':[20.5,20.5],\n",
    "           '46_C=C(C#N)C(C(OC)=C)=O':[11.8,11.8],\n",
    "           '47_O=C(/C(C)=C(C)/CC)C1=C(C)CCCC1':[13.9,13.8],\n",
    "           '48_O=C(/C(C)=C(C)/CC)C1=C(C(C)C)CCCC1':[15.0,14.0],\n",
    "           '49_O=C(/C(C)=C(C)/COC1=CC(OC)=CC(OC)=C1)/C(C)=C(C)/C2=CC=CC=C2':[13.5,13.5],\n",
    "           '50_O=C(/C(C)=C(C)/COC1=CC(OC)=CC(OC)=C1)/C(C)=C(C2=CC=CC=C2)/C':[15.6,15.6],\n",
    "           '51_O=C(/C(C)=C(COC1=CC(OC)=CC(OC)=C1)/C)/C(C)=C(C)/C2=CC=CC=C2':[10.4,8.7],\n",
    "           '57_O=C(/C(C)=C(C)\\C)C1=CCCC1':[14.7,14.7],\n",
    "           '58_C=CC(C1=CCCC1)=O':[18.3,16.5],\n",
    "           '59_O=C(/C(C)=C(C)\\C)C1=C(C)CCCO1':[14.7,14.5],\n",
    "           '60_O=C(C=C)C1=CCCCO1':[15.1,12.6],\n",
    "           '61_C=CC(C1=CNC=C1)=O':[23.1,22.0],\n",
    "           '62_C=CC(C1=CC=CN1)=O':[27.0,24.8],\n",
    "           '63_O=C(C1=CNC=C1)C2=CNC=C2':[33.1,32.7],\n",
    "           '64_C=CC(/C=C/C1=COC=C1)=O':[22.5,20.6],\n",
    "           '65_O=C(/C(C)=C(C)/C1=COC=C1)/C(C)=C(C)/C':[11.4,11.4],\n",
    "           '66_C=CC(C(C1=CC=CO1)=C)=O':[10.5,10.1],\n",
    "           '67_C=CC(C(C1=COC=C1)=C)=O':[11.3,10.7],\n",
    "           '68_C=CC(/C=C/[Si](C)(C)C)=O':[15.4,13.9],\n",
    "           '69_C=CC(C(P(OC)(OC)=O)=C)=O':[19.6,19.6],\n",
    "           '70_O=C(/C=C(C)/C1=CC=CC=N1)/C=C(C)\\C':[28.0,25.7],\n",
    "           '71_O=C(/C=C(C)/C1=CC=NC=C1)/C=C(C)\\C':[28.7,25.9],\n",
    "           '72_O=C(/C=C(C)/C1=CC=CN=C1)/C=C(C)\\C':[28.8,26.0],\n",
    "           '73_C=CC(C=C=C)=O':[15.8,12.6],\n",
    "           '75_C=CC(C([Si](C)(C)C)=C)=O':[12.9,12.9],\n",
    "           '76_C=CC(/C=C/P(OC)(OC)=O)=O':[18.4,16.6],\n",
    "           '77_O=C(C1=CCCC1)C2=CCCC2':[17.7,17.7],\n",
    "           '78_C=CC(C1=COC=C1)=O':[20.1,19.4],\n",
    "           '79_C=CC(C1=CC=CO1)=O':[28.3,25.7],\n",
    "           '80_O=C(/C(C)=C(C)/C)C1=COC=C1':[16.1,16.1],\n",
    "           '81_O=C(C1=CC=NC=C1)C2=CC=NC=C2':[36.0,36.0],\n",
    "           '82_O=C(C1=CC=NC=N1)C2=CC=NC=N2':[41.2,37.8],\n",
    "           '83_C=CC(C1=CC(OC)=CC(OC)=C1)=O':[17.9,17.5],\n",
    "           '84_O=C(/C(C)=C(C)/C)C1=CC(OC)=CC(OC)=C1':[13.5,13.5],\n",
    "           '85_C=CC(C(C1NCOC1=O)=C)=O':[25.1,24.6],\n",
    "           '86_O=C(/C(C1NCOC1=O)=C(C)/C)/C(C)=C(C)/C':[23.9,16.4],\n",
    "           '87_C=CC(/C=C\\C(C1=CC=CC=C1)C2=CC=CC=C2)=O':[19.4,16.1],\n",
    "           '89_O=C(/C(C)=C(C)\\C)/C(C(C1=CC=CC=C1)C2=CC=CC=C2)=C(C)\\C':[13.0,10.2],\n",
    "           '90_C=C(C1CCC1)C(C(C2CCC2)=C)=O':[10.8,10.8],\n",
    "           '92_C=CC(/C=C/C1=NC(C=CC=C2)=C2S1)=O':[20.7,18.3],\n",
    "           '93_C=CC(C1=CC=NC=C1)=O':[22.3,22.0],\n",
    "           '94_O=C(C(SC)=C)C1=CC=CC=C1':[19.2,19.2],\n",
    "           '95_C=CC(/C=C/C1=CC=NC=C1)=O':[20.4,18.4],\n",
    "           '96_C=CC(/C=C/C1=CC=C(Br)C=C1)=O':[20.7,19.3],\n",
    "           '97_O=C(/C(C)=C(C)/C)C1=C(C)CCC1':[18.9,18.6],\n",
    "           '98_O=C(C1=CC=CC=C1)C2=CC=C(C(F)(F)F)C=C2':[38.4,38.4],\n",
    "           '101_C=CC(C(C1CCC1)=C)=O':[13.5,13.3],\n",
    "           '102_O=C(/C(C)=C(C)/C)/C(C)=C(C)/C1CCC1':[13.9,13.9],\n",
    "           '103_C=CC(C1=CC=CS1)=O':[24.7,24.0],\n",
    "           '104_C=CC(/C=C/C(C1=CC=CC=C1)C2=CC=CC=C2)=O':[18.9,17.6],\n",
    "           '105_O=C(/C(C)=C(C)/C)C1=C(C)C=CS1':[24.5,23.5],\n",
    "           '106_O=C(C1=CC=CO1)C2=CC=CO2':[40.7,39.2],\n",
    "           '107_O=C(/C=C/C#N)/C=C/C#N':[23.5,22.1],\n",
    "           '108_C=C(C#C)C(C(C#C)=C)=O':[10.4,10.4],\n",
    "           '109_C=C(C)C(C(N)=C)=O':[14.3,7.4],\n",
    "           '110_C=C(C)C(C(NC)=C)=O':[14.8,6.7],\n",
    "           '113_C=C(F)C(C(F)=C)=O':[15.1,13.8],\n",
    "           '114_O=C(/C=C/F)/C=C/F':[21.2,19.8],\n",
    "           '115_O=C(/C=C\\F)/C=C\\F':[41.4,37.7],\n",
    "           '116_C=C(Br)C(C(Br)=C)=O':[10.6,10.6],\n",
    "           '117_O=C(/C=C/Br)/C=C/Br':[23.2,21.6],\n",
    "           '118_O=C(/C=C\\Br)/C=C\\Br':[39.0,35.2],\n",
    "           '119_C=C(C)C(C(C1NCOC1=O)=C)=O':[21.7,21.7],\n",
    "           '120_C=CC(C1=CC=NC=N1)=O':[21.1,21.1],\n",
    "           '121_C=C(C)C(C(C)=C=C)=O':[7.8,7.8],\n",
    "           '122_O=C(/C(C#C)=C(C)\\C)/C(C#C)=C(C)/C':[18.1,16.3],\n",
    "           '123_C=C(C)C(C1=CNC=C1)=O':[22.8,22.8],\n",
    "           '124_C=C(C)C(C1=CC=CN1)=O':[22.4,22.4],\n",
    "           '125_C=C(C)C(C1=COC=C1)=O':[17.0,17.0],\n",
    "           '126_C=C(C)C(C1=CC=CO1)=O':[23.0,23.0],\n",
    "           '127_O=C(/C(C)=C(C)\\C)C1=CC=CO1':[21.9,21.7],\n",
    "           '128_O=C(/C(C)=C(C)\\C)C1=CC=CN1':[21.0,20.9],\n",
    "           '129_O=C(/C(C)=C(C)/C)C1=CC=CS1':[21.1,20.5],\n",
    "           '130_C=C(C)C(C1=CC=CS1)=O':[21.3,21.3],\n",
    "           '131_C=C(C#N)C(C(SC)=C)=O':[8.3,8.3],\n",
    "           '132_C=C(C)C(C([Si](C)(C)C)=C)=O':[11.9,11.9],\n",
    "           '133_O=C(/C(C)=C/C1=COC=C1)/C(C)=C/C':[11.6,11.6],\n",
    "           '134_O=C(/C(C1=CC=CO1)=C(C)\\C)/C(C)=C(C)/C':[10.1,8.9],\n",
    "           '135_O=C(/C(C1=COC=C1)=C(C)/C)/C(C)=C(C)/C':[11.8,11.8],\n",
    "           '136_O=C(C1=CC=NC=N1)/C(C)=C(C)\\C':[23.7,23.7],\n",
    "           '137_O=C(/C(SC)=C(C)/C1=COC=C1)/C(C)=C(C)/C':[9.3,9.3],\n",
    "           '138_C=C(C)C(C1=CC(OC)=CC(OC)=C1)=O':[15.5,15.5],\n",
    "           '139_C=C(SC)C(C1=CC(OC)=CC(OC)=C1)=O':[15.5,15.5],\n",
    "           '141_C=C(C)C(/C(C)=C\\C(C1=CC=CC=C1)C2=CC=CC=C2)=O':[10.5,10.5],\n",
    "           '142_O=C(/C(SC)=C(C)/C#N)/C(C)=C(C)/C':[10.6,9.4],\n",
    "           '143_O=C(/C(SC)=C(C)/C)/C(C)=C(C)/C#N':[13.6,13.6],\n",
    "           '144_C=C(SC)C(C(SC)=C)=O':[7.4,7.4],\n",
    "           '147_O=C(/C(SC)=C/C)/C(SC)=C/C':[8.0,8.0],\n",
    "           '148_C=C(OC)C(C(OC)=C)=O':[11.7,10.8],\n",
    "           '149_O=C(/C(OC)=C/C)/C(OC)=C/C':[10.9,9.6],\n",
    "           '151_O=C(/C(F)=C(C)/C)/C(F)=C(C)/C':[19.4,18.3],\n",
    "           '152_O=C(/C(C)=C(C)/F)/C(C)=C(C)/F':[13.7,13.7],\n",
    "           '153_O=C(/C(C)=C(C)\\F)/C(C)=C(C)\\F':[23.4,23.4],\n",
    "           '154_O=C(/C(Br)=C(C)/C)/C(Br)=C(C)/C':[14.0,14.0],\n",
    "           '155_O=C(/C(C)=C(C)/Br)/C(C)=C(C)/Br':[12.9,12.4],\n",
    "           '156_O=C(/C(C)=C(C)\\Br)/C(C)=C(C)\\Br':[18.3,18.3],\n",
    "           '157_O=C(/C(C)=C(C)/C)/C(F)=C(C)/C':[16.0,15.9],\n",
    "           '158_C=C(F)C(C(C)=C)=O':[11.7,11.7],\n",
    "           '159_C=C(Br)C(C(C)=C)=O':[10.2,10.2],\n",
    "           '160_O=C(/C(C)=C(C)/C)/C(Br)=C(C)/C':[13.9,13.4],\n",
    "           '161_C=C(C#C)C(C=C)=O':[14.6,13.4],\n",
    "           '162_C=C(C#C)C(C(C#N)=C)=O':[12.7,12.7],\n",
    "           '163_C=C(C)C(C(C1=CC=CO1)=C)=O':[8.8,8.8],\n",
    "           '164_C=C(C)C(C(C1=COC=C1)=C)=O':[9.1,9.1],\n",
    "           '165_O=C(/C=C/C#C)/C=C/C#C':[24.4,22.4],\n",
    "           '166_O=C(/C(C)=C/C#C)/C(C)=C/C#C':[14.7,14.7],\n",
    "           '167_O=C(/C(C#N)=C(C#N)/C#N)/C(C#N)=C(C#N)/C#N':[34.4,32.3],\n",
    "           '168_O=C(/C(C)=C(C)/SC)/C(C)=C(C)/SC':[19.0,19.0],\n",
    "           '169_O=C(/C=C/SC)/C=C/SC':[30.5,28.8],\n",
    "           '170_C=C([Si](C)(C)C)C(C([Si](C)(C)C)=C)=O':[12.0,12.0],\n",
    "           '171_C=CC(/C=C/OC)=O':[23.3,21.8],\n",
    "           '172_C=CC(/C=C/C(OC)=O)=O':[19.5,16.7],\n",
    "           '173_C=CC(/C=C/SC)=O':[25.2,22.2],\n",
    "           '174_C=C(C)C(/C(SC)=C/C#N)=O':[9.1,9.1],\n",
    "           '175_C=C(C)C(/C(C#N)=C/SC)=O':[15.1,15.1],\n",
    "           '176_C=C(C)C(/C(SC)=C/F)=O':[9.2,9.2],\n",
    "           '177_C=C(C)C(/C(F)=C/SC)=O':[14.4,14.2],\n",
    "           '179_C=C(C)C(/C(Br)=C/SC)=O':[13.6,10.0],\n",
    "           '180_O=C(/C(SC)=C(C)/C)/C(C)=C(C)/F':[11.8,11.5],\n",
    "           '181_O=C(/C(SC)=C(C)\\C)/C(C)=C(C)/Br':[14.6,13.9],\n",
    "           '182_O=C(/C(SC)=C(C)/F)/C(C)=C(C)/C':[9.8,9.0],\n",
    "           '183_O=C(/C(SC)=C(Br)\\C)/C(C)=C(C)/C':[10.1,9.2],\n",
    "           '184_O=C(/C(C)=C(C(F)(F)F)\\C)/C(C)=C(C)/C':[14.2,13.5],\n",
    "           '186_O=C(/C(SC)=C(C(F)(F)F)\\C)/C(C)=C(C)/C':[11.9,10.5],\n",
    "           '187_O=C(/C(SC)=C(C(F)(F)F)\\C)/C(SC)=C(C)/C(F)(F)F':[10.4,8.6],\n",
    "           '189_O=C(/C(C(F)(F)F)=C(C)\\C)/C(C(F)(F)F)=C(C)/C':[23.9,21.6],\n",
    "           '190_O=C(/C(C(F)(F)F)=C/C)/C(C(F)(F)F)=C/C':[27.1,27.1],\n",
    "           '191_O=C(/C(C1=CC=CC=C1)=C(C)\\C)/C(C2=CC=CC=C2)=C(C)/C':[12.4,10.0],\n",
    "           '192_C=C(C1=CC=CC=C1)C(C(C2=CC=CC=C2)=C)=O':[9.9,9.9],\n",
    "           '193_O=C(/C(C1=CC=CC=C1)=C(C#N)\\C)/C(C2=CC=CC=C2)=C(C)/C#N':[14.5,11.6],\n",
    "           '195_C=C(SC)C(C(C1=CC=CO1)=C)=O':[5.9,5.9],\n",
    "           '196_C=C(SC)C(C(C1=COC=C1)=C)=O':[6.5,6.5],\n",
    "           '197_O=C(/C(O)=C(C)\\C)/C(C)=C(C)/C':[11.8,11.8],\n",
    "           '198_C=C(C)C(C(O)=C)=O':[10.6,10.6],\n",
    "           '199_C/C(C(C1=CC=CC=C1)=O)=C\\C':[22.9,22.9],\n",
    "           '200_C/C(C(C1=CC=CC=C1)=O)=C\\C#N':[24.7,24.7],\n",
    "           '201_O=C(/C(SC)=C/C#N)C1=CC=CC=C1':[19.5,19.5],\n",
    "           '202_C=C(SC)C(/C=C/C#N)=O':[11.9,10.5],\n",
    "           '203_C=C(C#N)C(/C=C/SC)=O':[25.2,24.4],\n",
    "           '204_O=C(/C(C#N)=C(C)/C)/C(C#N)=C(C)/C':[29.2,26.4],\n",
    "           '205_O=C(/C(SC)=C/C#N)/C(SC)=C/C#N':[8.4,8.4],\n",
    "           '206_O=C(/C(C#C)=C/C#N)/C(C#C)=C/C#N':[15.7,15.7],\n",
    "           '207_C=C(SC)C(C(SC)=C=C)=O':[4.7,4.7],\n",
    "           '208_O=C(/C(SC)=C/C(F)(F)F)/C(SC)=C/C(F)(F)F':[8.5,8.5],\n",
    "           '210_O=C(/C(C#N)=C\\C#N)/C(C#N)=C\\C#N':[28.6,27.4],\n",
    "           '213_C=C(C1=CC=CO1)C(C(C2=CC=CO2)=C)=O':[5.7,5.7],\n",
    "           '214_C=C(C)C(C(C#N)=C=C)=O':[8.8,8.8],\n",
    "           '216_O=C(C(C)=C=C)/C(C)=C/C#N':[10.7,10.7],\n",
    "           '217_C/C(C(C1=CC=CC=C1)=O)=C\\C(F)(F)F':[22.3,20.4],\n",
    "           '218_C/C(C(C1=CC=CC=C1)=O)=C\\F':[22.0,22.0],\n",
    "           '220_O=C(C=C=C)/C=C/Br':[18.6,15.1],\n",
    "           '221_O=C(C=C=C)/C=C/C#N':[18.1,15.0],\n",
    "           '222_O=C(C=C=C)/C=C/F':[17.4,14.7],\n",
    "           '223_O=C(/C(C1=CC=CC=C1)=C\\C)/C(C2=CC=CC=C2)=C\\C':[14.9,13.2],\n",
    "           '224_O=C(/C(C#C)=C\\C#N)/C(C#C)=C\\C#N':[24.4,20.1],\n",
    "           '225_C=C(O)C(C(O)=C)=O':[10.5,9.7],\n",
    "           '226_O=C(/C(SC)=C\\C)/C(SC)=C\\C':[10.9,10.7],\n",
    "           '227_O=C(/C=C\\C(F)(F)F)C=CC(F)(F)F':[20.1,18.0],\n",
    "           '228_O=C(/C=C/C(F)(F)F)/C=C/C(F)(F)F':[23.3,21.2],\n",
    "           '229_O=C(C=C=C)/C=C\\C#N':[22.6,19.5],\n",
    "           '230_O=C(C=C=C)/C=C\\F':[28.0,24.6],\n",
    "           '232_C=C(C)C(C(SC)=C=C)=O':[7.9,7.9],\n",
    "           '233_C=CC(C(SC)=C=C)=O':[11.2,9.1],\n",
    "           '234_O=C(C(SC)=C=C)/C=C/C#N':[12.9,10.2],\n",
    "           '235_O=C(C(SC)=C=C)/C=C\\C#N':[15.8,13.9],\n",
    "           '236_O=C(C(SC)=C=C)/C(C)=C\\C#N':[12.6,12.6],\n",
    "           '237_C=CC(/C(C1=COC=C1)=C/C#N)=O':[13.2,12.7],\n",
    "           '238_C=C(C1=COC=C1)C(/C=C/C#N)=O':[13.9,12.4],\n",
    "           '239_C=C(C1=COC=C1)C(/C=C\\C#N)=O':[15.6,14.9],\n",
    "           '240_C=C(SC)C(/C(C)=C\\F)=O':[23.0,14.0],\n",
    "           '241_C=C(F)C(/C(C)=C\\SC)=O':[13.8,8.8],\n",
    "           '242_C=C(C)C(/C(SC)=C\\C#N)=O':[12.0,11.8],\n",
    "           '243_C=C(C)C(/C(C#N)=C\\SC)=O':[22.0,17.8],\n",
    "           '244_C=C(C)C(/C(SC)=C\\F)=O':[15.9,15.9],\n",
    "           '246_C=C(SC)C(/C(C)=C\\C#N)=O':[11.6,11.6],\n",
    "           '247_C=C(C#N)C(/C(C)=C\\SC)=O':[22.0,19.0],\n",
    "           '248_C=C(SC)C(C1=CC=CS1)=O':[19.8,19.8],\n",
    "           '249_O=C(/C(C)=C\\C#N)C1=CC=CS1':[25.8,25.8],\n",
    "           '250_O=C(/C(C)=C/C#N)C1=CC=CS1':[24.7,24.7],\n",
    "           '251_O=C(/C(SC)=C\\C#N)C1=CC=CS1':[23.8,23.8],\n",
    "           '252_O=C(/C(SC)=C/C#N)C1=CC=CS1':[21.5,21.5],\n",
    "           '253_O=C(/C=C\\C#N)C1=CC=CO1':[33.2,30.5],\n",
    "           '254_O=C(/C=C/C#N)C1=CC=CO1':[33.0,29.3],\n",
    "           '255_O=C(/C=C\\F)C1=CC=CN1':[32.4,29.0],\n",
    "           '256_O=C(/C=C/F)C1=CC=CN1':[24.5,23.9],\n",
    "           '257_O=C(/C=C\\C#N)/C=C\\C#N':[31.2,29.4],\n",
    "           '258_O=C(/C(C)=C(C(F)(F)F)/C(F)(F)F)/C(C)=C(C(F)(F)F)/C(F)(F)F':[19.1,17.3],\n",
    "           '259_O=C(/C(C(F)(F)F)=C(C(F)(F)F)/C(F)(F)F)/C(C(F)(F)F)=C(C(F)(F)F)/C(F)(F)F':[25.6,19.1],\n",
    "           '261_C=CC(/C=C/F)=O':[18.7,17.2],\n",
    "           '262_C=CC(/C(SC)=C/F)=O':[12.3,10.9],\n",
    "           '263_C=C(SC)C(/C=C/F)=O':[10.4,10.1],\n",
    "           '264_C=CC(C(SC)=C)=O':[10.4,9.8],\n",
    "           '265_C=CC(/C(SC)=C/Br)=O':[13.3,11.9],\n",
    "           '267_C=CC(/C=C/Br)=O':[20.0,18.4],\n",
    "           '268_C=CC(/C(SC)=C\\F)=O':[20.0,18.4],\n",
    "           '269_C=C(SC)C(/C=C\\F)=O':[18.9,17.3],\n",
    "           '270_C=CC(/C=C\\F)=O':[29.1,26.4],\n",
    "           '271_C=CC(/C(SC)=C\\Br)=O':[17.3,15.2],\n",
    "           '272_C=C(SC)C(/C=C\\Br)=O':[18.5,15.2],\n",
    "           '273_C=CC(/C=C\\Br)=O':[27.6,23.8],\n",
    "           '274_O=C(/C(C1=COC=C1)=C/C#N)/C(C2=COC=C2)=C/C#N':[9.8,9.8],\n",
    "           '297_O=C(/C=C\\C#N)C1=CC=CS1':[29.2,28.5],\n",
    "           '298_O=C(/C=C/C#N)C1=CC=CS1':[28.6,27.3],\n",
    "           '299_C=C(C(F)(F)F)C(C(C(F)(F)F)=C)=O':[15.8,15.8],\n",
    "           '300_C=C(C#N)C(C(C#N)=C)=O':[13.9,13.9],\n",
    "           '301_C=CC(/C=C/C#N)=O':[20.1,18.0],\n",
    "           '302_C=CC(/C=C/C)=O':[19.2,17.5],\n",
    "           '303_O=C(/C(C)=C(C)/C(F)(F)F)/C(C)=C(C)/C(F)(F)F':[15.4,15.4],\n",
    "           '304_O=C(/C(C)=C\\F)/C(C)=C\\F':[25.2,25.2],\n",
    "           '305_O=C(/C(C)=C\\Br)/C(C)=C\\Br':[23.9,23.4],\n",
    "           '307_C=C(C)C(C(N1CCOC1=O)=C)=O':[6.9,6.9],\n",
    "           '309_O=C(/C(F)=C(C)/F)/C(F)=C(C)/F':[17.4,15.5],\n",
    "           '310_O=C(/C(F)=C(F)/C)/C(F)=C(F)/C':[26.9,26.5],\n",
    "           '311_O=C(/C(F)=C(F)/F)/C(F)=C(F)/F':[26.7,25.0],\n",
    "           '312_O=C(/C(Br)=C(C)/Br)/C(Br)=C(C)/Br':[14.0,12.9],\n",
    "           '313_O=C(/C(Br)=C(Br)/C)/C(Br)=C(Br)/C':[21.1,21.1],\n",
    "           '316_C=C(Cl)C(C(Cl)=C)=O':[10.4,10.4],\n",
    "           '317_O=C(/C=C/Cl)/C=C/Cl':[23.0,21.3],\n",
    "           '318_O=C(/C=C\\Cl)/C=C\\Cl':[40.2,36.6],\n",
    "           '319_O=C(/C(Cl)=C(C)/C)/C(Cl)=C(C)/C':[14.7,14.2],\n",
    "           '320_O=C(/C(C)=C(C)/Cl)/C(C)=C(C)/Cl':[13.9,13.9],\n",
    "           '321_O=C(/C(C)=C(C)\\Cl)/C(C)=C(C)\\Cl':[20.3,19.0],\n",
    "           '322_O=C(/C(C)=C(C)/C)/C(Cl)=C(C)/C':[13.4,13.0],\n",
    "           '323_C=C(Cl)C(C(C)=C)=O':[10.1,10.1],\n",
    "           '324_C=CC(/C=C/Cl)=O':[19.7,18.0],\n",
    "           '326_O=C(/C(C)=C/F)/C(C)=C/F':[12.3,12.3],\n",
    "           '327_O=C(/C(C)=C/Br)/C(C)=C/Br':[13.3,13.3],\n",
    "           '328_O=C(/C(C)=C/Cl)/C(C)=C/Cl':[13.1,13.1]\n",
    "           \n",
    "          }\n",
    "\n",
    "### Create Dataset\n",
    "dataset = {int(k.split('_')[0]):[k.split('_')[1]]+v for k,v in dataset.items()}\n",
    "dataset_clean = {k:v for k,v in dataset.items() if len(v)>1}\n",
    "\n",
    "\n",
    "#### Create various dictionaries for different properties that can be used as atom/node features\n",
    "enchart = {'C':2.55,'H':2.2,'N':3.04,'O':3.44,'F':3.98,'Si':1.9,'P':2.19,'Cl':3.16,'Br':2.96,'I':2.66,'S':2.58}\n",
    "enmin,enmax = min(enchart.values()), max(enchart.values())\n",
    "enchart = {k:(v-enmin)/(enmax-enmin) for k,v in enchart.items()}\n",
    "\n",
    "polchart = {'C':11.3,'H':4.5,'N':7.4,'O':5.3,'F':3.74,'Si':37.3,'P':25,'Cl':14.6,'Br':21,'I':32.9,'S':19.4}\n",
    "polmin,polmax = min(polchart.values()), max(polchart.values())\n",
    "polchart = {k:(v-polmin)/(polmax-polmin) for k,v in polchart.items()}\n",
    "\n",
    "masschart = {'C':12.01,'H':1.01,'N':14.01,'O':16.00,'F':18.00,'Si':28.09,'P':30.98,'Cl':35.45,'Br':79.90,'I':126.90,'S':32.07}\n",
    "massmin,massmax = min(masschart.values()), max(masschart.values())\n",
    "masschart = {k:(v-massmin)/(massmax-massmin) for k,v in masschart.items()}\n",
    "\n",
    "vdwchart = {x:Chem.GetPeriodicTable().GetRvdw(Chem.GetPeriodicTable().GetAtomicNumber(x)) for x in masschart.keys()}\n",
    "vdwmin,vdwmax = min(vdwchart.values()), max(vdwchart.values())\n",
    "vdwchart = {k:(v-vdwmin)/(vdwmax-vdwmin) for k,v in vdwchart.items()}\n",
    "\n",
    "covchart = {x:Chem.GetPeriodicTable().GetRcovalent(Chem.GetPeriodicTable().GetAtomicNumber(x)) for x in masschart.keys()}\n",
    "covmin,covmax = min(covchart.values()), max(covchart.values())\n",
    "covchart = {k:(v-covmin)/(covmax-covmin) for k,v in covchart.items()}\n",
    "\n",
    "for k,v in dataset_clean.items():\n",
    "    tempmol = Chem.MolFromSmiles(v[0])\n",
    "    AllChem.ComputeGasteigerCharges(tempmol)\n",
    "    charges = [x.GetDoubleProp('_GasteigerCharge') for x in tempmol.GetAtoms()]\n",
    "    cmin,cmax = min(charges), max(charges)\n",
    "\n",
    "for k,v in dataset_clean.items():\n",
    "    tempmol = Chem.MolFromSmiles(v[0])    \n",
    "    dmatrix = GetMoleculeBoundsMatrix(tempmol) ### use average bond length from min and max values\n",
    "    bondlens = [np.mean([dmatrix[b.GetBeginAtom().GetIdx()][b.GetEndAtom().GetIdx()],dmatrix[b.GetEndAtom().GetIdx()][b.GetBeginAtom().GetIdx()]]) for b in tempmol.GetBonds()]\n",
    "    bmin,bmax = min(bondlens), max(bondlens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17b5bd6",
   "metadata": {},
   "source": [
    "## 2.2) Model and Training/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa49fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class GraphConv3(torch.nn.Module):\n",
    "    'Graph Convolutional Network class with 3 convolutional layers and a linear layer'\n",
    "\n",
    "    def __init__(self, inputdimensions,dim_h):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(inputdimensions, dim_h)\n",
    "        self.conv2 = GraphConv(dim_h, dim_h)\n",
    "        self.conv3 = GraphConv(dim_h, dim_h)\n",
    "        self.lin = torch.nn.Linear(dim_h, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        e = data.edge_index\n",
    "        x = data.x\n",
    "\n",
    "        x = self.conv1(x, e)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, e)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, e)\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "\n",
    "        x = Fun.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "newmodel=GraphConv3\n",
    "epochs=2000\n",
    "dimh = 120\n",
    "batch_size = 32\n",
    "printresults=100\n",
    "AddHs=False\n",
    "pattern = ['C=CC(C=C)=O','C:CC(C:C)=O','C=CC(C:C)=O','C:CC(C=C)=O']\n",
    "pattneigh = 1\n",
    "lr=0.0001\n",
    "weight_decay=5e-4\n",
    "path = [f\"pretrained_GNN_models/GNN_model_fold{n}.pt\" for n in range(5)]\n",
    "pathNoPatt = [f\"pretrained_GNN_models/GNN_model_fold{n}.pt\" for n in range(5)]\n",
    "if not os.path.exists(os.path.dirname(path[0])):\n",
    "    os.mkdir(os.path.dirname(path[0]))\n",
    "if not os.path.exists(os.path.dirname(pathNoPatt[0])):\n",
    "    os.mkdir(os.path.dirname(pathNoPatt[0]))\n",
    "    \n",
    "\n",
    "### bfdic and afdic are useful for experimenting with different feature combinations\n",
    "bfdic={'bondtype':True,\n",
    "                'conjugated':True,\n",
    "                'inring':True,\n",
    "                'stereochemistry':False,\n",
    "                'bondlength':False}\n",
    "\n",
    "afdic = {'atomtypeonehot':True,\n",
    "                 'useHs':AddHs,\n",
    "                 'heavyatomValence':False,\n",
    "                 'formalcharge':False,\n",
    "                 'hybridization':False,\n",
    "                 'ringmembership':True,\n",
    "                 'aromatic':True,\n",
    "                 'electronegativity':True,\n",
    "                 'atompolarizability':False,\n",
    "                 'gasteiger':True,\n",
    "                 'mass':False,\n",
    "                 'vdw':False,\n",
    "                 'covalent':False,\n",
    "                 'chirality':False, \n",
    "                 'hydrogencount':False}\n",
    "\n",
    "# ### Run model using graph of reaction centre and nearest neighbours (Used in manuscript) \n",
    "# kfoldsresults = kfoldsGNN(list(dataset_clean.values()),model=newmodel,afdic=afdic,bfdic=bfdic,k_folds = 5,epochs = epochs,loss = torch.nn.MSELoss(),manseed=0,pattern =None,batch_size=batch_size,dim_h=dimh,printresults=printresults,AddHs=AddHs,neighbours=pattneigh,lr=lr,weight_decay=weight_decay,path=pathNoPatt)\n",
    "\n",
    "# ### Run model using entire molecule as graph (Not used but can be beneficial for extremely large molecules)\n",
    "# kfoldsresults_restrictedReactionCentre = kfoldsGNN(list(dataset_clean.values()),model=newmodel,afdic=afdic,bfdic=bfdic,k_folds = 5,epochs = epochs,loss = torch.nn.MSELoss(),manseed=0,pattern =pattern,batch_size=batch_size,dim_h=dimh,printresults=printresults,AddHs=AddHs,neighbours=pattneigh,lr=lr,weight_decay=weight_decay,path=path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78576819",
   "metadata": {},
   "source": [
    "# 3) Testing on New Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe7d25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### New unknown data in the format ['SMILES',BH]:\n",
    "## Since these will only be tested, the barrier heights can be any number, ex: 1000\n",
    "testdata = [['C=C(SC)C(C(SC)=C)=O',1000],['C(C(C1=CC=CC=C1)=O)=C',1000]]\n",
    "###\n",
    "\n",
    "### Train new models\n",
    "# testingpath = [f\"pretrained_GNN_models/GNN_272trainvalid_fold{n}.pt\" for n in range(5)]\n",
    "# kfoldsresults = test_new_molecule(list(dataset_clean.values()),testdata,model=newmodel,afdic=afdic,bfdic=bfdic,k_folds = 5,\n",
    "#                                   epochs = epochs,loss = torch.nn.MSELoss(),manseed=0,pattern =None,batch_size=batch_size,\n",
    "#                                   dim_h=dimh,printresults=printresults,AddHs=AddHs,neighbours=pattneigh,lr=lr,\n",
    "#                                   weight_decay=weight_decay,path=testingpath)\n",
    "# testingpath = [f\"pretrained_GNN_models/GNNrestrictedgraph_272trainvalid_fold{n}.pt\" for n in range(5)]\n",
    "# kfoldsresults = test_new_molecule(list(dataset_clean.values()),testdata,model=newmodel,afdic=afdic,bfdic=bfdic,k_folds = 5,\n",
    "#                                   epochs = epochs,loss = torch.nn.MSELoss(),manseed=0,pattern =pattern,batch_size=batch_size,\n",
    "#                                   dim_h=dimh,printresults=printresults,AddHs=AddHs,neighbours=pattneigh,lr=lr,\n",
    "#                                   weight_decay=weight_decay,path=testingpath)\n",
    "\n",
    "\n",
    "\n",
    "### The best paths of the best models will be printed during training and you can add these to the pretrainpaths list\n",
    "pretrainpaths = ['pretrained_GNN_models/GNN_272trainvalid_fold0_1452_20.77485_3.17891.pt',\n",
    "                 'pretrained_GNN_models/GNN_272trainvalid_fold1_1804_16.99289_3.05668.pt',\n",
    "                 'pretrained_GNN_models/GNN_272trainvalid_fold2_946_12.59959_2.28863.pt',\n",
    "                 'pretrained_GNN_models/GNN_272trainvalid_fold3_1832_18.58084_3.07982.pt',\n",
    "                 'pretrained_GNN_models/GNN_272trainvalid_fold4_1651_19.17843_2.82847.pt']\n",
    "kfoldsresults = test_new_molecule(list(dataset_clean.values()),testdata,model=newmodel,afdic=afdic,bfdic=bfdic,k_folds = 5,\n",
    "                                  epochs = epochs,loss = torch.nn.MSELoss(),manseed=0,pattern =None,batch_size=batch_size,\n",
    "                                  dim_h=dimh,printresults=printresults,AddHs=AddHs,neighbours=pattneigh,lr=lr,\n",
    "                                  weight_decay=weight_decay,path=testingpath,pretraining=pretrainpaths)\n",
    "\n",
    "pretrainpaths = ['pretrained_GNN_models/GNNrestrictedgraph_272trainvalid_fold0_1712_14.8546_2.76991.pt',\n",
    "                 'pretrained_GNN_models/GNNrestrictedgraph_272trainvalid_fold1_1543_16.17973_3.03558.pt',\n",
    "                 'pretrained_GNN_models/GNNrestrictedgraph_272trainvalid_fold2_948_9.77825_2.10556.pt',\n",
    "                 'pretrained_GNN_models/GNNrestrictedgraph_272trainvalid_fold3_1487_19.30715_2.89396.pt',\n",
    "                 'pretrained_GNN_models/GNNrestrictedgraph_272trainvalid_fold4_1940_16.4872_2.78486.pt']\n",
    "kfoldsresults = test_new_molecule(list(dataset_clean.values()),testdata,model=newmodel,afdic=afdic,bfdic=bfdic,k_folds = 5,\n",
    "                                  epochs = epochs,loss = torch.nn.MSELoss(),manseed=0,pattern =None,batch_size=batch_size,\n",
    "                                  dim_h=dimh,printresults=printresults,AddHs=AddHs,neighbours=pattneigh,lr=lr,\n",
    "                                  weight_decay=weight_decay,path=testingpath,pretraining=pretrainpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d39e39c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942e322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygeo",
   "language": "python",
   "name": "pygeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
